import os
import sys
from pyspark.sql import SparkSession

# --- CORRECCI√ìN PARA WINDOWS ---
# Aseguramos que PySpark use el mismo Python que est√° ejecutando este script.
os.environ['PYSPARK_PYTHON'] = sys.executable
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

# --- CONFIGURACI√ìN DE RUTAS ---
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
RUTA_PARQUET = os.path.join(BASE_DIR, "ejercicios_bigdata", "datos", "taxi.parquet")

def consulta_spark():
    """
    Ejemplo de consulta SQL usando Apache Spark.
    """
    if not os.path.exists(RUTA_PARQUET):
        print(f"‚ùå Error: No encuentro {RUTA_PARQUET}. Ejecuta el ejercicio 03 primero.")
        return

    print("‚ö° Iniciando SparkSession...")
    print("   (Si est√°s en Windows y ves advertencias sobre 'winutils', no te preocupes, es normal en local)")
    
    try:
        spark = SparkSession.builder \
            .appName("EjercicioBigDataSpark") \
            .master("local[*]") \
            .config("spark.ui.showConsoleProgress", "false") \
            .getOrCreate()
        
        # Reducir el nivel de log para no asustar al usuario con warnings de Hadoop
        spark.sparkContext.setLogLevel("ERROR")

        print(f"üìñ Leyendo Parquet con Spark: {RUTA_PARQUET}")
        df = spark.read.parquet(RUTA_PARQUET)
        
        df.createOrReplaceTempView("tabla_taxis")
        
        print("üß† Ejecutando consulta SQL...")
        query = """
        SELECT 
            payment_type,
            count(*) as total_viajes,
            ROUND(AVG(fare_amount), 2) as tarifa_promedio,
            ROUND(AVG(trip_distance), 2) as distancia_promedio
        FROM tabla_taxis
        GROUP BY payment_type
        ORDER BY payment_type
        """
        
        resultado = spark.sql(query)
        
        print("\nüìä Resultados de la consulta:")
        resultado.show()
        
        print("üõë Deteniendo SparkSession...")
        spark.stop()
        print("‚úÖ Fin del ejercicio.")
        
    except Exception as e:
        print("\n‚ùå Ocurri√≥ un error al ejecutar Spark.")
        print("Posible causa: En Windows, Spark necesita 'winutils.exe' para algunas operaciones de disco.")
        print(f"Detalle del error: {e}")

if __name__ == "__main__":
    consulta_spark()
